# Provider Configuration
# Options: openrouter (default), vllm (self-hosted)
# Default configuration uses OpenRouter for cloud-based inference
# For self-hosted setup with vLLM, use docker-compose.selfhosted.yml
PROVIDER=openrouter

# OpenRouter Configuration (default - cloud-based)
OPENROUTER_API_KEY=your_openrouter_api_key_here
OPENROUTER_API_BASE=https://openrouter.ai/api/v1
OPENROUTER_MODEL=qwen/qwen3-vl-8b-instruct

# vLLM Configuration (for self-hosted setup only)
# Only needed when using docker-compose.selfhosted.yml
MODEL_NAME=Qwen/Qwen2-VL-7B-Instruct
VLLM_HOST=vllm
VLLM_PORT=8000

# Hugging Face Token (required for downloading models when using vLLM)
HUGGING_FACE_HUB_TOKEN=your_huggingface_token_here

# API Configuration
MAX_FILE_SIZE_MB=10

# Token Usage Tracking
# Token usage data is automatically stored in SQLite database (token_usage.db)
# Access historical costs via GET /token-costs endpoint
# The database is created automatically on first run

# For production deployment, you might want to add:
# API_HOST=0.0.0.0
# API_PORT=8080
# FRONTEND_PORT=3000
